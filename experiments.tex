\section{Experiments}
We evaluate \ibsp{} in partially observable pick-and-place and navigation tasks.
The robot in our simulation is a full model of the Willow Garage PR2. We
provide results for several belief distributions: unimodal Gaussian, multimodal
Gaussian, and uniform.
\subsection{Observation Operator Specification and Generators}
The maximum likelihood component of the determinized problem
essentially follows the dynamics specified in
\secref{sec-formulation}. In order to extend this domain to handle
partial observability, we add the following fluents and operators:
\begin{tightlist}
  \item[Fluents:] \emph{MLInView}(r\_pose, o\_pose\_bel);
    \emph{MLOccludes}(obj, o\_pose\_bel, r\_obj\_pose);
    \emph{UncertainGP}(o\_pose\_bel, r\_pose, grasp);
    \emph{UncertainObstructs}(obj, traj)
  \item[]Operators: \\\emph{ObserveGP}(r\_obs\_pose, o, o\_pose\_bel)
\begin{tightlist}
  \item[\emph{pre}:] \emph{MLLoc}(o, o\_pose\_bel) $\wedge$
    \emph{MLRLoc}(r\_obs\_pose) \\$\wedge$ $\forall o_i
    \lnot$\emph{MLOccludes}($o_i$, o\_pose\_bel, r\_obs\_pose) \\ $\wedge$
    \emph{MLInView}(r\_obs\_pose, o\_pose)
  \item[\emph{eff}:] $\forall$ r\_p, g $\lnot$\emph{UncertainGP}(o\_pose\_bel, r\_p, g)
\end{tightlist}
\emph{ObserveObstruction}(r\_obs\_pose, traj, held\_obj, grasp) 

\begin{tightlist}
  \item[\emph{pre}:] \emph{MLRLoc}(r\_obs\_pose) \\$\wedge$
    \emph{MLInView}(r\_obs\_pose, traj)\\ $\wedge$
    $\lnot$\emph{MLOccludes}($o_i$, traj, r\_obs\_pose)
  \item[\emph{eff}:] $\forall o \lnot$\emph{UncertainObstructs}(o,
    traj)
\end{tightlist}
\end{tightlist}

The additional skolem functions needed to support this functionality
are \emph{ViewPose}(Obj) and \emph{ViewPose}(Traj) for each object and
trajectory in our skolemized representation. The generator for object
view poses draws samples from a 1-meter unit disc around the
object. The head pose is fixed to point at the object's maximum
likelihood position. The generator for trajectory view poses returns
the first step along the trajectory that has non-negligible probability of being
in collision. Then, we sample a viable base pose from along this trajectory.

\subsection{Observation model and state estimation for the PR2}
We model observations with ray casting from a simulated head-mounted
Kinect on our robot. For objects that are in the view
frustum, we get a false negative with probability proportional to the
amount of the object that is occluded. If we do get a positive
observation, we observe the object's true pose with Gaussian noise.


In order to successfully plan for this domain, it is important that we
effectively incorporate negative observations into our
updates. Without negative observations, it is impossible to determine
that a trajectory is clear without observing actual objects
locations. 

We account for negative observations with a factored representation for
the belief state. In one component, we maintain a distribution that
accounts for positive observations (e.g., a Kalman filter). The other component is an explicit
representation of the view cones associated with negative
observations. We refer to the first component as the positive
observation distribution and the second as the negative regions. To
sample from this distribution, we perform probabilistic rejection sampling:
we sample from the positive observation distribution and then discard
the sample in inverse proportion to the probability that it lies in a
negative observation region.


\subsection{Evaluations}
We implement our experiments in Python and use
OpenRave~\cite{Diankov_2008_6117} to represent the environment. We use
trajectory optimization for our motion planning~\cite{schulman2013finding}
and implement custom collision checking in Flexible Collision Library to handle
collisions. We use the Fast-Forward and Fast-Downward
task planners to solve our high-level planning problems. Because our system works
independently of the chosen task planner, we use Fast-Forward in domains involving negative
preconditions, which are not supported by Fast-Downward. We validate our approach in three
distinct scenarios.

To explore ensuring safety in robot navigation tasks under uncertainty, we
evaluate performance in a corridor domain. We place the robot at one end of a corridor
and present it with the goal of traversing the corridor, then grasping
objects on tables on the other side. Uniform distributions model the locations of
a varying number of obstructing columns throughout the corridor; thus, the solution
typically ends up as an alternating sequence of observations and motions. Note that this task would be essentially
insoluble without accounting for negative observations appropriately: without negative
observations, we might need to traverse the region we are checking in
order to view the object.

The second task demonstrates the ability to reason about occlusions. We
set a goal of grasping a target object from a cluttered table with other
objects on it: some have deterministic locations and others have a Gaussian distribution
modeling their location. The target object's location is also drawn from a Gaussian
distribution. \ibsp{} plans to remove any occlusions, observe the target object, and
then pick it up. In our experiments, we maintain that 3 of the objects on the table -- always including
the target object -- are part of the belief space.

The final scenario illustrates multimodal planning behavior applied to a setting
with three drawers, one of which contains a target object to be grasped. We use a
mixture of Gaussians to model the prior for the target's location.
Furthermore, we introduce a deterministic object in front of each drawer, which possibly
obstructs it from being opened. \ibsp{} begins by assuming the target is placed at its maximum likelihood
location; accordingly, it discovers that the target is obstructed by the drawer containing it, then
plans to open this drawer. If the outer deterministic object impedes the drawer from opening, this
obstruction fact is also discovered and accounted for. During execution, upon performing an observation inside the drawer, our system
chooses to refine the plan if the object was not actually seen (indicating that its true location is in another drawer).

Table \ref{table:results} shows timing results for the three described
tasks. \figref{fig:corridorimgs} and \figref{fig:drawerimgs} show intermediate steps and belief
states for the corridor and drawer tasks.

\begin{table}
  \centering
  \vspace{8pt}
  \begin{tabular}{lcc}
    \toprule
      Problem Type & \% Solved in 1800s & Avg Solution Time (s)\\
    \midrule
      Uniform corridor navigation, 1 obj & 100 & 232\\
    \midrule
      Uniform corridor navigation, 2 obj & 100 & 445\\
    \midrule
      Uniform corridor navigation, 3 obj & 88 & 926\\
    \midrule
      Uniform corridor navigation, 4 obj & 58 & 1168\\
    \midrule
      Unimodal search, 5 obj & 95 & 89\\
    \midrule
      Unimodal search, 10 obj & 95 & 162\\
    \midrule
      Unimodal search, 15 obj & 83 & 135\\
    \midrule
      Unimodal search, 20 obj & 70 & 229\\
    \midrule
      Unimodal search, 25 obj & 68 & 166\\
    \midrule
      Unimodal search, 30 obj & 48 & 122\\
    \midrule
      Multimodal 3-drawer search & 93 & 325\\
    \bottomrule
  \end{tabular}
  \caption{Solution percentages and running times for \ibsp{} in
    occluded search and navigation tasks. Unimodal cluttered table results are from 40
    randomly generated problems per number of objects. Multimodal drawer search results
    are from 30 iterations. Uniform corridor navigation results are from 40 randomly
    generated locations of corridor obstructions, per number of objects.}
  \label{table:results}
\end{table}



\begin{figure*}
  \centering
  \begin{subfigure}[b]{0.22\linewidth}
    \includegraphics[width=\textwidth]{corridor_images/0-observe.png}
    \caption{Initial belief state}
    \label{fig:step1}
  \end{subfigure}
  \begin{subfigure}[b]{0.22\linewidth}
    \includegraphics[width=\textwidth]{corridor_images/1-observe.png}
    \caption{after 1 observations}
    \label{fig:step2}
  \end{subfigure}
  \begin{subfigure}[b]{0.22\linewidth}
    \includegraphics[width=\textwidth]{corridor_images/2-observe.png}
    \caption{after 2 observations}
    \label{fig:step3}
  \end{subfigure}
  \begin{subfigure}[b]{0.22\linewidth}
    \includegraphics[width=\textwidth]{corridor_images/3-observe.png}
    \caption{after 3 observations}
    \label{fig:step4}
  \end{subfigure}
  \caption{Execution trace of interfaced belief space planning
    traversing a corridor with uncertain obstacles to pick an
    object. The blue cylinders are draws from the belief distribution
    during execution. After reaching the other side of the corridor,
    the robot will search amongst the three table locations in order
    to pick up its target object, whose distribution is a mixture of
    Gaussians. The high level representation for both the goal object
    and the obstructions are the same, despite the very different
    belief distributions.}
  \label{fig:experiment}
\end{figure*}

\begin{figure*}
  \centering
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\textwidth]{gaussianmix/init_state.png}
    \caption{Objects at ML location}
    \label{fig:step1}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\textwidth]{gaussianmix/initial_object.png}
    \caption{Initial belief state of target object}
    \label{fig:step2}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\textwidth]{gaussianmix/initial_obstructions.png}
    \caption{Initial belief state of obstructions}
    \label{fig:step3}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\textwidth]{gaussianmix/post_object.png}
    \caption{Belief state of target after 1 observe}
    \label{fig:step4}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\textwidth]{gaussianmix/post_obstructions.png}
    \caption{Belief state of obstructions after 1 observe}
    \label{fig:step5}
  \end{subfigure}
  \begin{subfigure}[b]{0.3\linewidth}
    \includegraphics[width=\textwidth]{gaussianmix/true_locs.png}
    \caption{True state of all objects}
    \label{fig:step6}
  \end{subfigure}
  \caption{Observations for a belief space execution
    to pick up an object, shown in pink, with a trimodal Gaussian distribution.
    In this environment the robot observes the current maximum likelihood location
    of the object, which is updated through positive and negative observations.}
  \label{fig:experiment}
\end{figure*}
