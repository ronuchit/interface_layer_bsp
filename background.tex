\section{Technical Background}
\subsection{Observable planning formulation}
\label{sec-formulation}
We use a STRIPS-style formalism to define our planning problems. We
introduce it with a pick-and-place task that will serve as a running
example for the paper.

We define a (fully observed) planning problem, $\Pi$ as a tuple, $\Pi
= \langle \E, \F, I, G, \Ops \rangle$, with the following definitions:
\begin{tightlist}
\item[$\E$:] a set of \emph{entities} within our domain; e.g., individual objects or locations.
\item[$\F$:] a set of \emph{fluents} that describe relations between
  entities.
\item[$I \in 2^\F$:] a conjunction of fluents that are initially true.
\item[$G \in 2^\F$:] a conjunction of fluents that characterizes the set of goal states.
\item[$\Ops$:] a set of parametrized \emph{operators} that describes
  ways the agent can alter the world. Each \emph{op} $\in \Ops$ is characterized by:
  \emph{preconditions}, \emph{pre(op)}, a conjunction of fluents that captures the set of states where an operator is applicable; and \emph{effects}, \emph{eff(op)}, which specifies the fluents that become true or false after executing \emph{op}.
\end{tightlist}
A solution to $\Pi$ is a sequence of operators and states, $\{(op_1,
s_1),\ldots,(op_N, s_N)\}$. To be a valid solution, the preconditions
for each operator must be satisfied in the state that precedes
it. $op_1$'s preconditions must be satisfied in the initial state and
the goal fluents must be satisfied in $s_N$. Subsequent states must
add the effects of the intermediate operator to the
previous state (i.e., $s_{i+1}$ is obtained by applying $op_i$'s effects to $s_i$). We
illustrate this with a specification of a pick-and-place task:
\begin{tightlist}
\item[$\E$:] objects ($o_i$, represented as strings), Null (a
  nonexistent object), object poses (continuous 6DOF), robot poses
  (continuous 20DOF), robot trajectories (continuous $20\cdot~T_{max}$ dimensional),
  and grasps (continuous 6DOF).
\item[$\F$:] \emph{Loc}(obj,~obj\_pose), \emph{RLoc}(r\_pose),
  \\\emph{Obstructs}(obj,~traj), \emph{Holding}(obj,~grasp),
  \\\emph{Connects}(r\_pose1,~r\_pose2,~traj),
  \\\emph{GraspPose}(obj,~obj\_pose,~r\_pose,~grasp).
\item[$\Ops$] \begin{tightlist}\item \emph{MoveTo}(r\_pose1, r\_pose2, traj) \begin{tightlist}
\item[\emph{pre}:] \emph{RLoc}(r\_pose1) $\wedge$
  \emph{Connects}(r\_pose1, r\_pose2, traj) $\wedge$ $\forall$i
  $\lnot$\emph{Obstructs}($o_i$, traj)
\item [\emph{eff}:] \emph{RLoc}(r\_pose2) $\wedge$ $\lnot$\emph{Rloc}(r\_pose1)
\end{tightlist}
\item \emph{Pick}(obj, obj\_pose, r\_pose, grasp)
\begin{tightlist}
   \item[\emph{pre}:] \emph{RLoc}(r\_pose) $\wedge$ \emph{Holding}(Null, Null) \\$\wedge$ \emph{GraspPose}(obj,~obj\_pose, r\_pose, grasp) \\$\wedge$ \emph{Loc}(obj, obj\_pose)
   \item[\emph{eff}:] \emph{Holding}(obj, grasp) $\wedge$ $\lnot$\emph{Loc}(obj, obj\_pose) \\$\wedge$ $\lnot$\emph{Holding}(Null, Null) \\$\wedge$ $\forall$t $\lnot$\emph{Obstructs}(obj, t)
\end{tightlist}
\item \emph{Place}(obj, obj\_pose, r\_pose, grasp)
\begin{tightlist}
   \item[\emph{pre}:] \emph{RLoc}(r\_pose) $\wedge$ \emph{Holding}(obj, grasp) \\ $\wedge$ \emph{GraspPose}(obj,~obj\_pose,~r\_pose,~grasp)
   \item[\emph{eff}:] $\lnot$\emph{Holding}(obj, grasp) $\wedge$ \emph{Loc}(obj, obj\_pose) \\$\wedge$ \emph{Holding}(Null, Null)\\ $\wedge \forall$ t s.t. \emph{overlaps}(obj, obj\_pose, t): \\ \indent \indent \ \ \ \emph{Obstructs}(obj, t)
\end{tightlist}
\end{tightlist}
\end{tightlist}
\subsection{Abstraction of continuous variables}
While this representation unambiguously specifies problems, the
presence of continuous parameters and conditional effects that depend
on these parameters (e.g., new obstructions from placing an object)
prevents direct solution with a classical planner. 

To use task planners in this setting, we abstract the continuous
aspects of our state in the style of Srivastava et
al.~\cite{srivastava2014combined}. The key step replaces continuous
variables by a discrete set of references to useful sets of continuous
values. These references are entities in a discrete formulation where
the initial state includes fluents that characterize these sets. This
process is called \emph{skolemization} and we refer to the introduced
entities as \emph{skolem variables}.

For example, in order to derive an abstract representation of the pick
operation for object $o_i$, we need a reference to the pose of the
object, the pose of the robot and the grasp it will use to pick it. To
abstract the pose of $o_i$ we introduce a new entity,
\emph{ObjPose}($o_i$), and include the fluent \emph{Loc}($o_i$,
\emph{ObjPose}($o_i$)) to characterize it. Intuitively, the new symbol
can be understood as `the pose that makes \emph{Loc}($o_i, \cdot$)
true.' We replace the robot pose and grasp with similar symbols,
parameterized by \emph{ObjPose}($o_i$), and include a \emph{GraspPose}
fluent that links the object identifier to the introduced
entities.

To capture the effects of actions that use these parameters, we use
\emph{sound} representations: facts which are guaranteed to become
true are included, but conditional effects that depend on the
particular binding of a skolem variable are not. For example, after
executing a place action, \emph{Loc}($o_i$, ObjPose($o_i$)) will always
be true, but the particular obstructions introduced will depend on the
exact binding of ObjPose($o_i$). We include the former in our
description and rely on an interface layer to discover conditional
effects (e.g., introduced obstructions) as they become relevant. The
process of assigning values to the variables of a high level plan is
called \emph{refinement} of the high level plan.

\begin{algorithm}
 \caption{An interface for deterministic problems} \label{alg-ifop}
 \begin{algorithmic}[1]
  \Procedure{InterfacePlan}{$S_0, \gamma$}
  \State $S \leftarrow S_0$
  \State $s \leftarrow $ExtractSymbols$(S_0)$
  \State // Keep track of the successfully refined plan prefix
  \State $p_{pre} \leftarrow None$ 
  \State $R_{pre} \leftarrow None$ 
  \State $p_{post} \leftarrow $ Plan$(s, \gamma)$// unrefined plan skeleton
  \While {not resource limit reached} 
     \State $R_{post} \leftarrow $ MotionPlan$(s, p_{post})$
     \State $is\_fail, i_{fail}, failure$\\\hspace{50pt}$ \leftarrow $ CheckSuccess$(S, (p_{post}, R_{post}))$
     \If {$\lnot is\_fail$}
         \State \Return $(p_{pre} + p_{post}, R_{pre} + R_{post})$
     \EndIf
     \State $p_{pre} \leftarrow p_{pre} + p_{post}[:i_{fail}]$
     \State $R_{pre} \leftarrow R_{pre} + R_{post}[:i_{fail}]$
     \State $S \leftarrow $ ApplyActions$(S_0, p_{pre}, R_{pre})$
     \State $s\leftarrow $ ExtractFailureSymbols$(S, failure)$
     \State $p_{post} \leftarrow $ Plan$(s, \gamma)$     
     \If {max attempts reached}
        \State reset pose generators
        \State reset $S, s, p_{post}, p_{pre}, R_{pre}$
     \EndIf
  \EndWhile
  \EndProcedure
 \end{algorithmic}
\end{algorithm}

\algoref{alg-ifop} shows pseudocode for such an interface layer. Lines
1-5 set up our planning problem. Line 6 constructs a high level
plan. Lines 8-10 attempt to find a motion plan for the high level
plan. If refinement fails, we identify a conditional effect of the
high level plan that may have caused failure, update our description,
and replan.
% This provides a random trajectory (as the refinement step is randomized) through the solution space. If we cannot find a solution, we reset random seeds and replan from the initial state.
This integration is complete for a subclass of planning
problems~\cite{srivastava2014combined}.



\subsection{Assumed maximum likelihood observations}
In this paper, we consider problems from a more general class than is
described in \secref{sec-formulation}---problems with partial
observability. To solve partially observable problems with
deterministic solvers, we use the \emph{maximum likelihood
  observation} (\mld) determinization of Platt et al.~\cite{platt2010belief}. Here
we give a brief description of their method.

The introduction of partial observability into a planning problem
associates each state with a distribution over observations. For
example, a state where a robot's depth sensor is pointed at an object
might give a noisy measurement of the object's pose. A solution is a
policy that maps a \emph{belief state}, a probability distribution
over states, to actions.

The central idea behind the \mld{} is that, if we fix the observation
for each belief state, updates to our belief are deterministic. For
example, belief dynamics with Gaussian state distributions and
Gaussian noise are defined, for each observation, by the Kalman filter
update equations. Fixing the observation for each state reduces this
to a deterministic update. While there are many determinizations of
belief space problems, the benefit of \mld{} is that it is a
deterministic problem that generates information-seeking behavior.
