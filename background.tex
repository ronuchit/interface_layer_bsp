\section{Technical Background}
\subsection{Observable planning formulation}
\label{sec-formulation}
We use a STRIPS-style formalism to define our planning problems. We
introduce a simple pick and place task that will serve as a running
example for the paper. 

Formally, we define a (fully observed) planning problem, $\Pi$ as a tuple, $\Pi =
\langle \E, \F, I, G, \Ops \rangle$, with the following definitions:
\begin{tightlist}
\item[$\E$:] a set of \emph{entities} within our domain; e.g., individual objects or locations.
\item[$\F$:] a set of \emph{fluents} that describe relations between
  entities.
\item[$I \in 2^\F$:] a conjunction of fluents that are initially true.
\item[$G \in 2^\F$:] a conjunction of fluents that characterizes the set of goal states.
\item[$\Ops$:] a set of parametrized \emph{operators} that describes
  ways the agent can alter the world. Each \emph{op} $\in \Ops$ is characterized by:
  \emph{preconditions}, \emph{pre(op)}, a conjunction of fluents that captures the set of states where an operator is applicable; and \emph{effects}, \emph{eff(op)}, which specifies the fluents that become true or false after executing \emph{op}.
\end{tightlist}
A solution to $\Pi$ is a sequence of operators and states, $\{(op_1,
s_1),\ldots,(op_N, s_N)\}$, such that preconditions are satisfied and
the goal is achieved. That is, $s_i~=~apply(eff(op_i),~s_{i-1});~
pre(op_i)~\in~s_{i-1},$ $pre(op_0)~\in~I;~s_N~\in~G$. We now give
a specification for our pick and place task:
\begin{tightlist}
\item[$\E$] objects ($o_i$, represented as strings), Null (a
  nonexistent object), object poses (continuous 6DOF), robot poses
  (continuous 20DOF), robot trajectories (continuous $20\cdot~T_{max}$ dimensional),
  and grasps (continuous 6DOF).
\item[$\F$] \emph{Loc}(obj,~obj\_pose), \emph{RLoc}(r\_pose),
  \\\emph{Obstructs}(obj,~traj), \emph{Holding}(obj,~grasp),
  \\\emph{Connects}(r\_pose1,~r\_pose2,~traj),
  \\\emph{GraspPose}(obj,~obj\_pose,~r\_pose,~grasp).
\item[$\Ops$] \begin{tightlist} \item \emph{MoveTo}(r\_pose1,~r\_pose2, traj)
\begin{tightlist}
   \item[\emph{pre}:] \emph{RLoc}(r\_pose1) \\$\wedge$ \emph{Connects}(r\_pose1,~r\_pose2,~traj) \\$\wedge$ $\forall i \lnot Obstructs(o_i,~traj)$
   \item[\emph{eff}:] \emph{RLoc}(r\_pose2) $\wedge$ $\lnot$\emph{RLoc}(r\_pose1)
\end{tightlist}
\item \emph{Pick}(obj, obj\_pose, r\_pose, grasp)
\begin{tightlist}
   \item[\emph{pre}:] \emph{RLoc}(r\_pose) $\wedge$ \emph{Holding}(Null, Null) \\$\wedge$ \emph{GraspPose}(obj,~obj\_pose, r\_pose, grasp) \\$\wedge$ \emph{Loc}(obj, obj\_pose)
   \item[\emph{eff}:] \emph{Holding}(obj, grasp) $\wedge$ $\lnot$\emph{Loc}(obj, obj\_pose) \\$\wedge$ $\lnot$\emph{Holding}(Null, Null) \\$\wedge$ $\forall$t $\lnot$\emph{Obstructs}(obj, t)
\end{tightlist}
\item \emph{Place}(obj, obj\_pose, r\_pose, grasp)
\begin{tightlist}
   \item[\emph{pre}:] \emph{RLoc}(r\_pose) $\wedge$ \emph{Holding}(obj, grasp) \\ $\wedge$ \emph{GraspPose}(obj,~obj\_pose,~r\_pose,~grasp)
   \item[\emph{eff}:] $\lnot$\emph{Holding}(obj, grasp) $\wedge$ \emph{Loc}(obj, obj\_pose) \\$\wedge$ \emph{Holding}(Null, Null)\\ $\wedge \forall$ t s.t. \emph{overlaps}(obj, obj\_pose, t): \\ \indent \indent \ \ \ \emph{Obstructs}(obj, t)
\end{tightlist}
\end{tightlist}
\end{tightlist}
\subsection{Abstraction of continuous variables}
While this representation unambiguously specifies problems, the
presence of continuous parameters and conditional effects that depend
on these parameters (e.g., new obstructions from placing an object)
prevents directly passing this representation to a classical
planner. We will need to compile our specification into a discrete
problem that classical planners can readily solve.

We achieve this through the abstraction of continuous aspects of our
state in the style of Srivastava et
al.~\cite{srivastava2014combined}. The key step in this approach
replaces continuous variables by a discrete set of references to useful
potential values. These references are included as objects in
the domain description with appropriate facts for the symbolic planner
to interpret them.

For example, in order to derive an abstract representation of the pick
operation for object $o_i$, we need a reference to the pose of the
object, pose of the robot and the grasp we will use to pick it. We represent these as
functions: \emph{ObjPose}($o_i$), \emph{RPose}($r$), \emph{Grasp}($o_i$, $r$). These are represented to the
classical planner as standard discrete objects; we then declare facts
about these variables that are guaranteed to be true. This process is
referred to as \emph{skolemization}.

\begin{algorithm}
 \caption{An interface for deterministic problems} \label{alg-ifop}
 \begin{algorithmic}[1]
  \Procedure{InterfacePlan}{$S_0, \gamma$}
  \State $S \leftarrow S_0$
  \State $s \leftarrow $ExtractSymbols$(S_0)$
  \State $p_{pre} \leftarrow None$
  \State $R_{pre} \leftarrow None$
  \State $p_{post} \leftarrow $ Plan$(s, \gamma)$
  \While {not resource limit reached} 
     \State $R_{post} \leftarrow $ MotionPlan$(s, p_{post})$
     \State $is\_fail, i_{fail}, failure$\\\hspace{50pt}$ \leftarrow $ CheckSuccess$(S, (p_{post}, R_{post}))$
     \If {$\lnot is\_fail$}
         \State \Return $(p_{pre} + p_{post}, R_{pre} + R_{post})$
     \EndIf
     \State $p_{pre} \leftarrow p_{pre} + p_{post}[:i_{fail}]$
     \State $R_{pre} \leftarrow R_{pre} + R_{post}[:i_{fail}]$
     \State $S \leftarrow $ ApplyActions$(S_0, p_{pre}, R_{pre})$
     \State $s\leftarrow $ ExtractFailureSymbols$(S, failure)$
     \State $p_{post} \leftarrow $ Plan$(s, \gamma)$     
     \If {max attempts reached}
        \State reset pose generators
        \State reset $S, s, p_{post}, p_{pre}, R_{pre}$
     \EndIf
  \EndWhile
  \EndProcedure
 \end{algorithmic}
\end{algorithm}

To capture the effects of actions that use these parameters, we use
\emph{sound} representations: facts which are guaranteed to become
true are included, but conditional effects are not. These are effects
that depend on the particular binding of a skolem variable. For
example, after executing a place action we know that \emph{Loc}($o_i$,
ObjPose($o_i$)) will be true, but the particular obstructions introduced
will depend on the exact bindings of ObjPose($o_i$). We include the
former in our description and rely on an interface layer to discover
conditional facts, such as the latter, as they become relevant and update the high level
description accordingly. The process of assigning values to the variables
of a high level plan is called \emph{refinement} of the high level
plan.

\algoref{alg-ifop} shows pseudocode for such an interface layer. Lines
1-5 set up our planning problem. Line 6 constructs a high level
plan. Lines 8-10 attempt to find a motion plan for the high level
plan. If refinement fails, we identify a fact that was missing from
the high level plan that may have caused failure, and the high level then
replans from that step.
% This provides a random trajectory (as the refinement step is randomized) through the solution space. If we cannot find a solution, we reset random seeds and replan from the initial state.
This integration is complete for a subclass of planning
problems~\cite{srivastava2014combined}.



\subsection{Assumed maximum likelihood determinization}
In this paper, we consider problems from a more general class than is
described in \secref{sec-formulation} -- problems with partial
observability. To solve partially observable problems with
deterministic solvers, we use the \emph{maximum
  likelihood determinization} (\mld)~\cite{platt2010belief}.

The introduction of partial observability into a planning problem
associates with each state a distribution over observations. For
example, a state where a robot's depth sensor is pointed at an object
might give a noisy measurement of the object's pose. A solution for
this problem is a policy that maps a \emph{belief state}, a
probability distribution over states, to actions.

The central idea behind the \mld{} is that, if we fix the observation
for each state, the Bayesian updates to our belief are
deterministic. For example, in the Gaussian case, belief dynamics are
defined by the Kalman filter update equations, and fixing the
observation for each state reduces this to a deterministic
update. While there are many determinizations of belief space
problems, the benefit of the \mld{} is that it is a deterministic
problem that still generates information-seeking behavior.

%% Smaller determinized problems can be solved with LQR approaches or
%% direct SQP optimization~\cite{platt2010belief,
%%   patil2014scaling}. These solutions are then used in a model
%% predictive control paradigm---a trajectory is computed, the first step
%% is executed, the true next belief state is observed, and the process
%% repeats. Our approach will apply this paradigm to larger plans that
%% leverage task and motion planning.
