\section{Interfaced belief space planning}
In this section, we present our primary contribution: \ibsp, a modular
approach to task and motion planning in belief
space. \secref{sec-bsp-formulation} extends our formulation of the
problem to include partial observability. \secref{sec-optimism}
details an approach based on optimistic observations to obtain a
useful abstraction of belief space dynamics.  \secref{sec-ibsp}
describes the \ibsp{} algorithm in detail.

%%PA: this entire section is very technical; it'd be really nice if
%%the intro (or otherwise here, but probably the intro) had a
%%motivating example that illustrates the representation / algorithm
%%in action, and illustrates its strengths; this would bring the
%%reader here with already most of the intuition about our approach;
%%right now the reader arrives with limited intuition, and is just
%%getting a lot of formal stuff to read here...; 

\subsection{Belief space planning formulation}
\label{sec-bsp-formulation}
In our full description of the problem, we need to allow for
probabilistic representations in belief space. We restrict ourselves
to problems where the uncertainty is confined to the continuous state
(e.g., we may not know $o_1$'s location, but we know its name and
whether we are holding it). The entities for a partially observed
version of our pick-and-place domain are distributions over object poses,
observations, and the entities described in
\secref{sec-formulation}. We replace the fluents from our pick-and-place
domain with belief fluents that are true if the corresponding
(deterministic) fluent holds with high probability. For example,
we have a fluent, \emph{BGraspPose}, which is true if \emph{GraspPose}
is true with high enough probability. Our input specifies a
distribution over observations that is conditioned on the state.

We divide our operators into \emph{actions}, which alter the true
state, and \emph{observations}, which do not alter the true state but
garner information about it. Actions are deterministic in belief space
(even when the underlying dynamics are not) and follow a similar
parametrization to the deterministic case. Observation operators are
parameterized by the belief about state variables that determine the
observation distribution.

This dichotomy restricts non-determinism to observations, so
determinizing observations obtains a representation suitable for task
planning. In principle, one can directly determinize the problem in
this representation and apply skolemization to get a sound description
of abstract belief dynamics. However, such an approach will ofter fail
to generate meaningful high-level plans.

We illustrate this issue with a belief space formulation of the
\emph{Pick} operator. To simplify description, we represent objects in
the frame of the robot and assume that the robot's pose in its own
frame is known.

In the full (intractable) representation, we have the following
description:\\ 
\emph{BPick}(o, o\_pose\_bel, r\_pose, grasp)
\begin{tightlist}
\item[\emph{pre:}] \emph{BLoc}(o, o\_pose\_bel) $\wedge$
  \emph{RLoc}(r\_pose) \\$\wedge$ \emph{BGraspPose}(o, o\_pose\_bel,
  r\_pose, grasp) \\$\wedge$ \emph{Holding}(Null, Null)
\item[\emph{eff}:] \emph{Holding}(o, grasp) $\wedge$
  $\lnot$\emph{BLoc}(o, o\_pose\_bel) \\$\wedge$
  $\lnot$\emph{Holding}(Null, Null) \\$\wedge$ $\forall$t, h, g
  $\lnot$\emph{BObstructs}(o, t, h, g)\\$\wedge$ $\forall$ o\_p,
  r\_p $\lnot$\emph{BOccludes}(o, o\_p, r\_p)
\end{tightlist}
The first precondition captures that the continuous parameter
o\_pose\_bel represents our current belief about the pose of o. The
problematic precondition is \emph{BGraspPose}. The issue is that this
a conditional effect of observation: it can only become true if
properties of the belief state that depend on continuous values are
true. The practical implication is that a task plan generated from
such a representation would never select an observation
action\footnote{In \cite{srivastava2014combined} conditional effects
  only appear as negative preconditions and this allows sounds
  representations to generate meaningful plans.}. Formulating a sound
logic over general belief states dynamics that enables this reasoning
is a challenging task and easily results in cumbersome and intractable
representations.

\subsection{Optimism for observations}
\label{sec-optimism}
The insight we use to solve this issue is that belief updates for
\mld{} often exhibit restricted dynamics: they will typically
concentrate belief at the maximum likelihood state for a
distribution. An action operator, on the other hand, can change the
maximum likelihood state for a distribution (and potentially
introduce variance). We need a formulation that
allows the task planner to effectively select between these
options. Thus, our approach reformulates a problem in terms of two
types of belief state fluents: those which characterize the maximum
likelihood state, and those which characterize the certainty in our
belief. The latter of the two shares similarities with the
BVLoc($\epsilon$) fluents from Kaelbling and
Lozano-P\'erez~\cite{kaelbling2013integrated}.

Returning to the preconditions for \emph{BPick}, there are
two reasons \emph{BGraspPose} could fail to hold: either 1) the
maximum likelihood state doesn't contain a valid grasp pose or 2) the
maximum likelihood state admits at least 1 valid grasp pose, but the
distribution has too much variance to allow a successful grasp with
high probability. In the first case, we must manipulate the state
(e.g., move obstructions), while in the other we must observe it. We
replace belief fluents with maximum likelihood fluents and uncertainty
fluents:\\ \emph{MLPick}(o, o\_pose\_bel, r\_pose, grasp)
\begin{tightlist}
\item[\emph{pre:}] \emph{MLLoc}(o, o\_pose\_bel) $\wedge$
  \emph{RLoc}(r\_pose) \\$\wedge$ \emph{MLGraspPose}(o,
  o\_pose\_bel, r\_pose, grasp) \\$\wedge$
  $\lnot$\emph{UncertainGP}(o\_pose\_bel, r\_pose, grasp)\\$\wedge$
  \emph{Holding}(Null, Null)
\item[\emph{eff}:] \emph{Holding}(o, grasp) $\wedge$
  $\lnot$\emph{MLLoc}(o, o\_pose\_bel) \\$\wedge$
  $\lnot$\emph{Holding}(Null, Null) \\$\wedge$ $\forall$t, h, g
  $\lnot$\emph{MLObstructs}(o, t, h, g)\\ $\wedge$ $\forall$ o\_p,
  r\_p $\lnot$\emph{MLOccludes}(o, o\_p, r\_p)
\end{tightlist}
In order to achieve $\lnot$\emph{UncertainGP} facts, we include an
observation operator:\\
\emph{ObserveGP}(r\_obs\_pose, o, o\_pose\_bel)
\begin{tightlist}
  \item[\emph{pre}:] \emph{MLLoc}(o, o\_pose\_bel) $\wedge$
    \emph{RLoc}(r\_obs\_pose) \\$\wedge$ $\forall o_i
    \lnot$\emph{MLOccludes}($o_i$, o\_pose\_bel, r\_obs\_pose) \\ $\wedge$
    \emph{MLInView}(r\_obs\_pose, o\_pose\_bel)
  \item[\emph{eff}:] $\forall$ r\_p, g $\lnot$\emph{UncertainGP}(o\_pose\_bel, r\_p, g)
\end{tightlist}
We call this an optimistic observation because the actual effect is
not guaranteed. We rely on the interface layer to discover cases when
it does not hold and correct our representation. Note that this
assumption of optimism is only made in the high level description. The
full algorithm plans with a complete belief representation: if
multiple observations are needed to successfully localize an object,
the algorithm will plan to take them them. In the event that our plan
was overly optimistic, BCheckSuccess on line 10 of \algoref{alg-ibsp}
(described in the next section) will detect the overly optimistic
behavior and trigger replanning.

\subsection{The \ibsp{} algorithm}
\label{sec-ibsp}
In this section, we detail the necessary changes to the interface from
Srivastava et al. to handle our belief space formulation. The primary
change is that the refinement operation must find an assignment of
continuous variables that maximizes the probability of success,
instead of simply satisfying constraints. \algoref{alg-ibsp} shows
pseudocode with changes to \algoref{alg-ifop} highlighted in red.

\begin{algorithm}
 \caption{The \ibsp{} planning algorithm} \label{alg-ibsp}
 \begin{algorithmic}[1]
  \Procedure{IBSP}{$B_0, \gamma, \epsilon$}
  \State $B \leftarrow B_0$
  \State $b \leftarrow $ ExtractSymbols$(B_0, None)$
  \State $p_{post} \leftarrow $ Plan$(b, \gamma)$
  \State $p_{pre} \leftarrow None$
  \State $R_{pre} \leftarrow None$
  \While {not resource limit reached} 
     \State $R_{post} \leftarrow $ \textcolor{red}{MLRefine}$(B, p_{post})$
     \State $is\_fail,i_{fail}, failure$\\\ \ \ \ \ \ \ \ \ \ $\leftarrow $ \textcolor{red}{BCheckSuccess}$(B, (p_{post}, R_{post}), \epsilon)$
     \If {$\lnot is\_fail$}
         \State \Return $(p_{pre} + p_{post}, R_{pre} + R_{post})$
     \EndIf
     \State $p_{pre} \leftarrow p_{pre} + p_{post}[:i_{fail}]$
     \State $R_{pre} \leftarrow R_{pre} + R[:i_{fail}]$
     \State $B \leftarrow $ \textcolor{red}{MLFilter} $(B, p_{pre}, R_{pre})$
     \State $b\leftarrow $ ExtractFailureSymbols$(B, failure)$
     \State $p_{post} \leftarrow $ Plan$(b, \gamma)$     
     \If {max attempts reached}
        \State reset pose generators
        \State reset $B, b, p_{post}, p_{pre}, R_{pre}$
     \EndIf
  \EndWhile
  \EndProcedure
  \Procedure{IBSP-execute}{$B, \gamma, world, \epsilon$}
    \State $(p, R) \leftarrow IBSP(B, \gamma, \epsilon)$
    \For {$i \in len(p)$}
      \State $B \leftarrow$ ExecuteAndFilter$(p[i], R[i], B, world)$
      \If{$\lnot$BCheckSuccess$(B, (p[i+1:], R[i+1:]))$}
        \State \Return IBSP-execute$(B, \gamma, world, \epsilon)$
      \EndIf
    \EndFor
  \EndProcedure
 \end{algorithmic}
\end{algorithm}

We take as input a safety parameter, $\epsilon$. This determines the
acceptable probability of failure for each step in the plan. The first
change replaces \emph{MotionPlan} with \emph{MLRefine}. Both
functions return a dictionary mapping skolem variables to continuous
values. The difference is that \emph{MotionPlan} does collision-free
motion planning to connect successive states, while \emph{MLRefine}
minimizes the probability of collision. \emph{MLRefine} can be defined
either by direct trajectory optimization to minimize an unnormalized
likelihood of collision, or by sampling objects from the belief state
and minimizing the number of samples that are in collision. The
implementation for our experiments adopts the second approach.

The second change is the introduction of \emph{BCheckSuccess}, which
examines a refinement and checks that preconditions are
satisfied. \algoref{alg-bcheck} shows pseudocode for this method. We
define preconditions with a function that maps a state to true or
false and indicates whether the given relationship holds.  We
determine that each of these is satisfied with probability greater
than $1-\epsilon$ by Monte Carlo sampling from the belief
state. $N_{samples}$ determines the fidelity of this sampling.

When we detect that a precondition does not hold, we must decide which
type of failure information to propagate to the high level. We do this
by checking if the predicate holds in the maximum likelihood state
(Line 7). If it does, then our error is caused by uncertainty in the
belief state, and we propagate an uncertainty failure. Otherwise,
we return a maximum likelihood fluent as the violated
precondition. The exceptions to this rule are predicates about
observations (e.g., occlusions). These are deterministic in an \mld{}
approximation, so it does not make sense to propagate uncertainty
failures for them.

\begin{algorithm}
 \caption{Determining failure or success of a refinement} \label{alg-bcheck}
 \begin{algorithmic}[1]
  \Procedure{BCheckSuccess}{$B, p, R, \epsilon$}
      \State // Iterate over the sequence of high level actions      
      \State // and their refinements
      \For{$p_i, R_i \in $ zip($p, R$)} 
          \State preds $\leftarrow $ ExtractPreconds($p_i, R_i$)
          \State W $\leftarrow$ Sample($B_i$, $N_{samples}$)
          \For{pred $\in$ preds}
              \If{ PercentSucceeds(pred, W) $< 1-\epsilon$}
                 \If {$\lnot$pred.eval(B.ML) or IsObs(pred)}
                     \State \Return (False, i, $\lnot$ML(pred))
                 \Else
                     \State \Return (False, i, Uncertain(pred))
                 \EndIf
              \EndIf
          \EndFor
          \State // update B with the results of this action
          \State B $\leftarrow$ MLFilter$(B, p_i, R_i)$
      \EndFor
      \State \Return (True, -1, None)
  \EndProcedure
 \end{algorithmic}
\end{algorithm}


The third change is the addition of \emph{MLFilter}, which updates the
belief state based on the maximum likelihood observation that
occurred. This is a natural extension of ApplyActions from
\algoref{alg-ifop} and is implemented with a black-box state
estimation method.

An important observation is that these implementations only require us
to be able to sample from our belief distribution and compute
unnormalized likelihoods. In order to compute the \mld{}, we need to
be able to compute a maximum likelihood observation and maintain a
filtered distribution. These are generic operations for belief states
and are implemented for many state estimation techniques. Lightweight
dependence on the form of the belief distribution allows one to switch
state estimators with no change to the algorithm. Thus, \ibsp{} is
a belief space planning algorithm that operates with black box access
to the belief updates. As a result, our experiments run on several
different initial state distributions with no modification to the
planning code.

%% We can extend the completeness proof from Srivastava et
%% al.~\cite{srivastava2014combined} to provide conditions that the
%% \mld{} for a planning problem will be solved by \ibsp{}. 
%% \begin{thm}
%% Let $P$ be a partially observable planning problem with deterministic
%% transitions. If the fully observed version of $P$ satisfies
%% \emph{InterfacePlan}'s conditions for completeness, then
%% \algoref{alg-ibsp} will find a solution to the \mld{} of $P$ with
%% error rate $\epsilon$ as $N_{samples}\rightarrow \infty$, provided
%% that all the calls to motion planning return.
%% \end{thm}
%% \begin{proof} (sketch) 
%% Deterministic state transitions imply that, once uncertainty fluents
%% become false that they never become true again (i.e., the domain is
%% uniform with respect to uncertainty fluents). The remaining predicates
%% characterize the fully observed problem and satisfy the conditions for
%% completeness by assumption. In the limit of infinite samples, we are
%% able to exactly evaluate whether predicates are satisfied. These
%% combine to satisfy the conditions to apply Theorem 1 from
%% \cite{srivastava2014combined}.
%% \end{proof}
